# -*- coding: utf-8 -*-
"""Final_Mindx_De1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gz3CuuDjB-kcNd1PVdC_YnvUDSZMXj4n

# Nguyễn Đức Đạt
#Nhóm 3 - DA14
#Đề số 1

# Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
#Import Library
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from scipy import stats
from scipy.stats import randint

# prep
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.datasets import make_classification
from sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler

# models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier

# Validation libraries
from sklearn import metrics
from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve
from sklearn.model_selection import cross_val_score


#ensemble
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

from sklearn.impute import SimpleImputer

#Library label encoder
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

#import dataset
df = pd.read_csv('/content/ex1.csv')

"""# <b> <font color ='green' >1. Explore Data Analysis </font> <b>

## <b> <font color ='green' >1.1 Overall Info </font> <b>
"""

df.head()

df.tail()

df.info()

df.describe()

"""## <b> <font color ='green' >1.2 Clean missing values

### <b> <font color ='green' >1.2.a Check Null values
"""

df.isnull().sum()

#% Null values
dict_null = dict()
for i in df.columns:
  dict_null[i] = df[i].isnull().sum()/len(df['Timestamp'])*100
df1 = pd.DataFrame.from_dict(dict_null.items())
print(df1)

df.drop(columns = ['Timestamp','state','Country','comments'], inplace = True)

df.isnull().sum()

"""### <b> <font color ='green' >1.2.b Clean missing values of self_employed column"""

df['self_employed'].unique()

df['self_employed'].value_counts()

df['self_employed'].replace(np.NaN,'No',inplace=True)

df['self_employed'].unique()

"""### <b> <font color ='green' >1.2.c Clean missing values of work_interfere column"""

df['work_interfere'].unique()

df['work_interfere'].value_counts()

df['work_interfere'].replace(np.NaN, "Don't Know",inplace = True)

df['work_interfere'].value_counts()

df.isnull().sum()

"""## <b> <font color ='green' >1.3 Checking values of all columns"""

my_list = df.columns.values.tolist()

for column in my_list:
  print(column)
  print(df[column].unique())

"""### <b> <font color ='green' >1.3.a  Age Column"""

from matplotlib.pyplot import figure

figure(figsize=(10, 10))
df['Age'].value_counts().plot( kind= 'bar')

outliers =[]
for age in df['Age'].values:
  if age < 0 or age >100 :
    outliers.append(age)
    print(outliers)

#Because There is only 5 outliers comparing total 1259 entries, so we can remove values of outliers

df = df.loc[(df['Age'] > 18) & (df['Age'] <100)]

# 0 values means no outliers 
print(df[df["Age"].isin(outliers)] )

#Grouping Age
Age_Group = pd.cut(df['Age'],bins=[17,23,30,61,100],labels=['18-22', '23-30 ','30-60', '> 61'])
df.insert(23,'Age_Group',Age_Group)
df['Age_Group'].unique()

"""### <b> <font color ='green' >1.3.b  Gender Column"""

df1= df['Gender'].unique()
print(df1)

male_string = ["M", "Male", "male", "m", "Male-ish", "maile", "Cis Male", "Mal", "Male (CIS)","Make", "Male ", "Man","msle", "Mail", "cis male","Malr","Cis Man"]
female_string = ["Female", "female", "Cis Female", "F","Woman",  "f", "Femake","woman", "Female ", "cis-female/femme","Female (cis)","femail"]
others_string = ["Trans-female", "something kinda male?", "queer/she/they", "non-binary","Nah", "all", "Enby", "fluid", "Genderqueer", "Androgyne", "Agender", "male leaning androgynous", "Guy (-ish) ^_^", "Trans woman", "Neuter", "Female (trans)", "queer", "ostensibly male, unsure what that really means"]           

for index, row in df.iterrows():

    if str(row.Gender) in male_string:
        df['Gender'].replace(to_replace=row.Gender, value='male', inplace=True)

    if str(row.Gender) in female_string:
        df['Gender'].replace(to_replace=row.Gender, value='female', inplace=True)

    if str(row.Gender) in others_string:
        df['Gender'].replace(to_replace=row.Gender, value='other', inplace=True)


print(df['Gender'].unique())

df.info()

"""# <b> <font color ='green' >2. Preprocessing - Encoding </font> <b> 

"""

label_dict = {}
#Label-Enconding
le = preprocessing.LabelEncoder()
for feature in df.columns:
  if feature != 'Age':
    le.fit(df[feature])
    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    df[feature] = le.transform(df[feature])
    # Get labels
    labelKey = 'label_' + feature
    labelValue = [*le_name_mapping]
    label_dict[labelKey] =labelValue
  else:
    label_dict['label_Age'] = list(df['Age'])

df.info()
df.head()

for key, value in label_dict.items():     
    print(key, value)

"""# <b> <font color ='green' >3. Covariance Matrix. Variability comparison between categories of variables </font> <b> 

"""

#correlation matrix
corrmat = df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);
plt.show()

#treatment correlation matrix
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'treatment')['treatment'].index
cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

"""# <b> <font color ='green' >4. Some charts to see data relationship
 </font> <b> 
"""

# Age & Treatment

g = sns.FacetGrid(df, col ='treatment', height=8)
g = g.map(sns.countplot, "Age_Group")

for ax in g.axes.flat:
    labels = ax.get_xticklabels() # get x labels
    for i,l in enumerate(labels):
        if(i == 0): labels[i] = '18-22'
        elif(i ==1.0):labels[i] = '23-30'
        elif(i ==2.0):labels[i] = '31-60'
        elif(i ==3.0):labels[i] = '> 61'  
    ax.set_xticklabels(labels, rotation=30) # set new labels
plt.show()

#Gender & Treatment
df1 = df
df1['Gender'] = df1['Gender'].astype('category')
print(df1['Gender'].unique())
plt.figure(figsize=(12,8))
g = sns.FacetGrid(df1, col='treatment', height=8)
g.map(sns.countplot,'Gender')

for ax in g.axes.flat:
    labels = ax.get_xticklabels() # get x labels
    for i,l in enumerate(labels):
        if(i == 0): labels[i] = 'Female'
        elif(i ==1):labels[i] = 'Male'
        else: labels[i] ='Other'  
    ax.set_xticklabels(labels, rotation=30) # set new labels
plt.show()

#Draw a catplot to show Percentage treatment for family_history by Gender

g = sns.catplot(x="family_history", y="treatment", hue="Gender", data=df, kind="bar",  ci=None, size=5, aspect=2, legend_out = True)

for ax in g.axes.flat:
    labels = ax.get_xticklabels() # get x labels
    for i,l in enumerate(labels):
        if(i == 0): labels[i] = 'No'
        else: labels[i] ='Yes'
    ax.set_xticklabels(labels, rotation=30) # set new labels

# title
g._legend.set_title('Gender')
new_labels = ['Female', 'Male', 'Other']
# replace labels
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

plt.title('Probability of health condition by family_history and Gender')
plt.ylabel('Probability x 100')
plt.xlabel('Family History')

#Draw a catplot to show Percentage treatment for Work_interfere by Gender

g = sns.catplot(x="work_interfere", y="treatment", hue="Gender", data=df, kind="bar",  ci=None, size=5, aspect=2, legend_out = True)

for ax in g.axes.flat:
    labels = ax.get_xticklabels() # get x labels
    for i,l in enumerate(labels):
        if(i == 0): labels[i] = "Don't Know" 
        elif(i ==1):labels[i] = 'Never'
        elif(i ==2):labels[i] = 'Often'
        elif(i ==3):labels[i] = 'Rarely'
        else: labels[i] = 'Sometimes'
    ax.set_xticklabels(labels, rotation=30) # set new labels

# title
g._legend.set_title('Gender')
new_labels = ['Female', 'Male', 'Other']
# replace labels
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

g.fig.subplots_adjust(top=1,right=0.8)
plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('work_interfere')

#Draw a catplot to show Percentage treatment for Care Benefit by Gender

g = sns.catplot(x="benefits", y="treatment", hue="Gender", data=df, kind="bar",  ci=None, size=5, aspect=2, legend_out = True)

for ax in g.axes.flat:
    labels = ax.get_xticklabels() # get x labels
    for i,l in enumerate(labels):
        if(i == 0): labels[i] = "Don't Know" 
        elif(i ==1):labels[i] = "No"
        else: labels[i] = "Yes"
    ax.set_xticklabels(labels, rotation=30) # set new labels

# title
g._legend.set_title('Gender')
new_labels = ['Female', 'Male', 'Other']
# replace labels
for t, l in zip(g._legend.texts, new_labels):
    t.set_text(l)

g.fig.subplots_adjust(top=1,right=0.8)
plt.title('Probability of mental health condition')
plt.ylabel('Probability x 100')
plt.xlabel('Care Options')

"""# <b> <font color ='green' >5. Scaling and fitting

## <b> <font color ='green' >5.1 Scaling
"""

#Scale Age
# We use MinMaxScaler instead of StandardScaler, RobustScaler because those 2 options leading scaling 'Age' to negative values , and this is disadvantage for models.  
from sklearn.preprocessing import MinMaxScaler 

scaler = MinMaxScaler ()
df['Age'] = scaler.fit_transform(df[['Age']])

df.head()

"""## <b> <font color ='green' >5.2 Fitting


"""

y = df['treatment']
X = df.drop(columns='treatment')


# split dataset to test and training set (75% train, 25% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

"""# <b> <font color ='green' >6. Tuning

## <b> <font color ='green' >6.1 Writing Evaluate Model Function
"""

methodDict = {} # This would be used for plotting the model's performance


# Validation libraries
from sklearn import metrics
from sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve,classification_report
from sklearn.model_selection import cross_val_score

def EvaluateModel(model, y_test, y_pred, plot=False):
    
    #Confusion matrix
    # save confusion matrix and slice into four pieces
    confusion = metrics.confusion_matrix(y_true =y_test, y_pred = y_pred)
  

    # visualize Confusion Matrix
    sns.heatmap(confusion,annot=True,fmt="d") 
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    #Metrics computed from a confusion matrix
    #Classification Accuracy: Overall, how often is the classifier correct?
    accuracy = metrics.accuracy_score(y_test, y_pred)
    print('Classification Accuracy:', accuracy)
    
    #Classification Error: Overall, how often is the classifier incorrect?
    print('Classification Error:', 1 - metrics.accuracy_score(y_test, y_pred))
    
    #Classification Report
    print('Classification Accuracy:' ,classification_report(y_test,y_pred))
    
  
    
    methodDict[model.__class__.__name__] = metrics.accuracy_score(y_test, y_pred) * 100

"""## <b> <font color ='green' >6.2 Tuning

"""

# Because dataset is small, I still would like to use Random Search instead of Bayes, or gridsearch because I want to minimize the tuning time and better result,. In this case : I use RandomizedSearchCV 
# Reference to https://towardsdatascience.com/gridsearch-vs-randomizedsearch-vs-bayesiansearch-cfa76de27c6b 


from sklearn.model_selection import KFold

kf = KFold(n_splits = 5, shuffle = True, random_state = 2)

def RandomSearch(model, param_dist):
  reg_bay = RandomizedSearchCV(estimator=model,
                    param_distributions=param_dist,
                    n_iter=20,  # search 20 times 
                    cv=kf,
                    n_jobs=8,
                    scoring='accuracy',
                    random_state =3)
  reg_bay.fit(X_train,y_train)
  y_pred = reg_bay.predict(X_test)
  print('RandomSearch. Best Score: ', reg_bay.best_score_)
  print('RandomSearch. Best Params: ', reg_bay.best_params_)
  accuracy_score = EvaluateModel(model, y_test, y_pred, plot =True)

"""# <b> <font color ='green' >7. Evaluate Models

## <b> <font color ='green' >7.1 Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
    
# make class predictions for the testing set
y_pred = logreg.predict(X_test)
    
print('########### Logistic Regression ###############')
    
accuracy_score = EvaluateModel(logreg, y_test, y_pred, plot =True)

"""## <b> <font color ='green' >7.2 K-Neighbors

"""

model = KNeighborsClassifier()

param_dist = {'n_neighbors': list(range(1,31)),
              'weights' :['uniform', 'distance']}

RandomSearch(model, param_dist)

"""## <b> <font color ='green' >7.3 Decision-Tree

"""

model_2 = DecisionTreeClassifier()
param_dist = {'max_depth': list(range(3, 9)),
              "max_features": list(range(1, len(X.columns))),
              "min_samples_split": list(range(2, 9)),
              "min_samples_leaf": list(range(1, 9)),
              "criterion": ["gini", "entropy"]} 

RandomSearch(model_2, param_dist)

"""## <b> <font color ='green' >7.4 RandomForest

"""

model_3 = RandomForestClassifier()
estimators = [int(x) for x in np.linspace(start = 1, stop = 100, num = 10)]
param_dist = {'n_estimators' : estimators,
             'max_depth': list(range(3, 9)),
              "max_features": list(range(1, len(X.columns))),
              "min_samples_split": list(range(2, 9)),
              "min_samples_leaf": list(range(1, 9)),
              "criterion": ["gini", "entropy"]} 
RandomSearch(model_3, param_dist)

"""## <b> <font color ='green' >7.5 AdaBoosting

"""

tree = DecisionTreeClassifier(max_depth = 1)
model = AdaBoostClassifier(base_estimator= tree, n_estimators= 100,random_state = 4)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

EvaluateModel(model, y_test, y_pred, True)

"""## <b> <font color ='green' >7.6 GradientBoosting

"""

model = GradientBoostingClassifier(n_estimators =100, max_depth =1,random_state = 5 )
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

EvaluateModel(model, y_test, y_pred, True)

"""## <b> <font color ='green' >7.7 Bagging

"""

tree = DecisionTreeClassifier(max_depth = 1)
model = BaggingClassifier(base_estimator = tree,max_samples=1.0, max_features=1.0, bootstrap_features=False, n_estimators = 100,random_state = 6)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

EvaluateModel(model, y_test, y_pred, True)

"""# <b> <font color ='green' >8. Success method plot

"""

s = pd.Series(methodDict)
s = s.sort_values(ascending=False)
plt.figure(figsize=(12,8))

ax = s.plot(kind='bar') 
for p in ax.patches:
  ax.annotate(str(round(p.get_height(),2)), (p.get_x() * 1.005, p.get_height() * 1.005))
plt.ylim([70.0, 90.0]) 
plt.xticks(rotation = 45)
plt.xlabel('Method')
plt.ylabel('Percentage')
plt.title('Success of methods')
     
plt.show()

"""# <b> <font color ='green' >9. Creating predictions on test set

"""

tree = DecisionTreeClassifier(max_depth = 1)
model = BaggingClassifier(base_estimator = tree,max_samples=1.0, max_features=1.0, bootstrap_features=False, n_estimators = 100,random_state = 6)

model.fit(X_train, y_train)
dfTestPredictions = model.predict(X_test)

# Write predictions to csv file
results = pd.DataFrame({'Index': X_test.index, 'predict_Treatment': dfTestPredictions,'test_treatment': y_test})
# Save to file
# This file will be visible after publishing in the output section
results.to_csv('results.csv', index=False)
print(results)

"""# <b> <font color ='green' >10. Saving model

"""

from google.colab import drive
drive.mount('/content/drive')

modeltosave = model

import _pickle as cPickle
import os
with open('/content/drive/MyDrive/Mindx_final/DucDat_De1_model.pkl', 'wb') as f:
    cPickle.dump(modeltosave, f)

print('Saved Model')